{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1488d6f-8d7f-4c34-883f-ea979b1e2b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:41.604994Z",
     "iopub.status.busy": "2025-08-15T15:51:41.604638Z",
     "iopub.status.idle": "2025-08-15T15:51:43.272646Z",
     "shell.execute_reply": "2025-08-15T15:51:43.271915Z",
     "shell.execute_reply.started": "2025-08-15T15:51:41.604971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installing Dependencies\n",
    "# bitsandbytes : Lightweight CUDA wrappers for k‑bit quantization\n",
    "# peft : (e.g., LoRA) o adapt large pretrained models by training a small set of extra parameters instead of all weights, reducing compute and storage\n",
    "# trl : Tools to post‑train/align transformers with RLHF and related methods\n",
    "\n",
    "!pip3 install bitsandbytes peft trl accelerate datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60131c91-9c1b-4c8b-a7bb-68c4cf0ed0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:46.145159Z",
     "iopub.status.busy": "2025-08-15T15:51:46.144792Z",
     "iopub.status.idle": "2025-08-15T15:51:46.148888Z",
     "shell.execute_reply": "2025-08-15T15:51:46.148290Z",
     "shell.execute_reply.started": "2025-08-15T15:51:46.145134Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, GemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980e2386-eedb-4192-8b6e-9570f023845e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:51:48.480563Z",
     "iopub.status.busy": "2025-08-15T15:51:48.480214Z",
     "iopub.status.idle": "2025-08-15T15:51:48.483678Z",
     "shell.execute_reply": "2025-08-15T15:51:48.483148Z",
     "shell.execute_reply.started": "2025-08-15T15:51:48.480542Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e450b89-b8aa-425e-a6a1-fb01222e60d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:52:00.205458Z",
     "iopub.status.busy": "2025-08-15T15:52:00.205033Z",
     "iopub.status.idle": "2025-08-15T15:52:00.210393Z",
     "shell.execute_reply": "2025-08-15T15:52:00.209892Z",
     "shell.execute_reply.started": "2025-08-15T15:52:00.205429Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Load the model in 4‑bit quantized weights to reduce GPU Memory \n",
    "    bnb_4bit_quant_type=\"nf4\",              # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16   # Perform computations in bfloat16 while keeping weights in 4‑bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb66e9e8-e166-4dcb-84b1-80a109c4ab50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:52:09.590958Z",
     "iopub.status.busy": "2025-08-15T15:52:09.590388Z",
     "iopub.status.idle": "2025-08-15T15:52:51.835433Z",
     "shell.execute_reply": "2025-08-15T15:52:51.834870Z",
     "shell.execute_reply.started": "2025-08-15T15:52:09.590933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b5aa7dfb24690af9940a435d01bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ec7c8f48864ba398acbbec36da9258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9945803fdc094279a3f30545f5732925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747523a0aa6a4a87b827ab2d1f7b4ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683bd489d5a2459c92c0ffb3f9738dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6579caccd645ff918ba6792e8e4cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c06a770ae4483a383b9ea6615fcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e36ce469e246edbe916ead93fe2f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3403767bba4c465095ac46c3101bc28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c3e7fc59bb4597acf98a3df8457002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833f3a7dc4194dff9eb970b48b259f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loads the correct tokenizer for Gemma‑2B from the Hugging Face Hub.\n",
    "# The same vocabulary/subword rules the model was trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ['HF_TOKEN'])\n",
    "\n",
    "# Downloads/loads the causal language model weights for model_id\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\":0},                     # Places all model modules on single GPU\n",
    "                                             token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d565fb-3293-47be-9c1e-b6e0209c7b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:53:48.990464Z",
     "iopub.status.busy": "2025-08-15T15:53:48.990129Z",
     "iopub.status.idle": "2025-08-15T15:53:50.909016Z",
     "shell.execute_reply": "2025-08-15T15:53:50.908409Z",
     "shell.execute_reply.started": "2025-08-15T15:53:48.990442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Imagination is more, than knowledge.\n",
      "\n",
      "I am a self-taught artist, born in 1985 in\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: Imagination is more,\"\n",
    "device = \"cuda:0\"                                            # Chooses GPU 0 for computations\n",
    "\n",
    "# Tokenizes the text into PyTorch tensors and moves those tensors onto GPU 0\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49541a3a-8ad6-407b-9d63-d0730b4ef71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:54:07.501132Z",
     "iopub.status.busy": "2025-08-15T15:54:07.500266Z",
     "iopub.status.idle": "2025-08-15T15:54:08.514658Z",
     "shell.execute_reply": "2025-08-15T15:54:08.514108Z",
     "shell.execute_reply.started": "2025-08-15T15:54:07.501105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Imagination is more important than knowledge. Knowledge is limited. Imagination encircles the world.\n",
      "\n",
      "- Albert Einstein\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: Imagination is more\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8290a3-c023-4b62-bd01-aa3f20bae6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:55:11.485387Z",
     "iopub.status.busy": "2025-08-15T15:55:11.485025Z",
     "iopub.status.idle": "2025-08-15T15:55:11.488500Z",
     "shell.execute_reply": "2025-08-15T15:55:11.487962Z",
     "shell.execute_reply.started": "2025-08-15T15:55:11.485362Z"
    }
   },
   "outputs": [],
   "source": [
    "# It sets a Weights & Biases setting via environment variable\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ff8601-98a6-493d-a084-277c34937118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:55:18.970164Z",
     "iopub.status.busy": "2025-08-15T15:55:18.969805Z",
     "iopub.status.idle": "2025-08-15T15:55:18.973258Z",
     "shell.execute_reply": "2025-08-15T15:55:18.972682Z",
     "shell.execute_reply.started": "2025-08-15T15:55:18.970140Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c698e-c1f0-4377-b379-c7b2eeb1ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    --> The LoRA rank (size of the low‑rank adapters). Higher r increases adapter capacity (and memory/compute), \n",
    "        lower r is lighter. r=8 is a common, efficient default.\n",
    "    --> target_modules : Specifies which linear layers to apply LoRA to, covers both attention and Feed Forwad Layers\n",
    "    --> Tells PEFT the task is causal language modeling, so it wires the adapters appropriately for an autoregressive decoder model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a7eee1-4e72-433a-8bff-9d6fc8b95ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:56:37.199934Z",
     "iopub.status.busy": "2025-08-15T15:56:37.199615Z",
     "iopub.status.idle": "2025-08-15T15:56:42.530492Z",
     "shell.execute_reply": "2025-08-15T15:56:42.529820Z",
     "shell.execute_reply.started": "2025-08-15T15:56:37.199911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95572840db34a0f9cb628c05c7d37a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03db3a48e91a439691a0fa36879a6b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d2a50cb1f145489deeccc208627d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e91a8fe84246b985b8b9211cfdbe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c9891b-b877-43aa-93ea-d24670156195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:57:06.200063Z",
     "iopub.status.busy": "2025-08-15T15:57:06.199720Z",
     "iopub.status.idle": "2025-08-15T15:57:06.205550Z",
     "shell.execute_reply": "2025-08-15T15:57:06.205005Z",
     "shell.execute_reply.started": "2025-08-15T15:57:06.200041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['“Be yourself; everyone else is already taken.”', \"“I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.”\", \"“Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.”\", '“So many books, so little time.”', '“A room without books is like a body without a soul.”'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['quote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d58d668-8651-4166-81f6-c909c811f9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:57:37.085511Z",
     "iopub.status.busy": "2025-08-15T15:57:37.085065Z",
     "iopub.status.idle": "2025-08-15T15:57:37.089576Z",
     "shell.execute_reply": "2025-08-15T15:57:37.089001Z",
     "shell.execute_reply.started": "2025-08-15T15:57:37.085477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns a list with one formatted string so the trainer can tokenize it internally\n",
    "def formatting_func(example):\n",
    "    text = f\"Quote: {example['quote'][0]}\\nAuthor: {example['author'][0]}\"\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc1155b-22de-4b24-a1e1-4e2a5337b36e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:57:50.459873Z",
     "iopub.status.busy": "2025-08-15T15:57:50.459549Z",
     "iopub.status.idle": "2025-08-15T15:57:50.463987Z",
     "shell.execute_reply": "2025-08-15T15:57:50.463462Z",
     "shell.execute_reply.started": "2025-08-15T15:57:50.459849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2508\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accesses the training split from the loaded/tokenized dataset\n",
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1deb5a7e-167b-4e72-8da1-99f3a2ab4d4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:58:27.734828Z",
     "iopub.status.busy": "2025-08-15T15:58:27.734508Z",
     "iopub.status.idle": "2025-08-15T15:58:31.757990Z",
     "shell.execute_reply": "2025-08-15T15:58:31.757326Z",
     "shell.execute_reply.started": "2025-08-15T15:58:27.734806Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:687: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a formatting function. Therefore `formatting_func` will be ignored. Either remove the `formatting_func` or pass a dataset that is not already processed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a894065c5734061b17843d01e16dcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,                            # Gamma model (with 4‑bit quant + LoRA adapters)\n",
    "    train_dataset=data[\"train\"],            # Trains on the train split\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,      # One sample per GPU step\n",
    "        gradient_accumulation_steps=4,      # Accumulates 4 steps → effective batch size≈4 \n",
    "        warmup_steps=2,                     # Small warmup before reaching full learning rate\n",
    "        max_steps=100,                      # Run 100 optimization steps total\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,                          # Use float16 for faster, lower‑memory training\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"            # Memory‑efficient 8‑bit AdamW optimizer (works well with bitsandbytes)\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba093d5-ced9-432b-bc03-91b2ccaa4951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T15:59:11.370102Z",
     "iopub.status.busy": "2025-08-15T15:59:11.369727Z",
     "iopub.status.idle": "2025-08-15T16:01:17.320607Z",
     "shell.execute_reply": "2025-08-15T16:01:17.320094Z",
     "shell.execute_reply.started": "2025-08-15T15:59:11.370079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.878600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.958500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.829600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.186500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.739500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.470200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.580300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.343700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.0560099977254866, metrics={'train_runtime': 114.0965, 'train_samples_per_second': 3.506, 'train_steps_per_second': 0.876, 'total_flos': 189744345784320.0, 'train_loss': 2.0560099977254866})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1562b9e8-be19-481f-adb7-f984a07a7dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:01:44.365828Z",
     "iopub.status.busy": "2025-08-15T16:01:44.365458Z",
     "iopub.status.idle": "2025-08-15T16:01:45.783087Z",
     "shell.execute_reply": "2025-08-15T16:01:45.782488Z",
     "shell.execute_reply.started": "2025-08-15T16:01:44.365805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: A woman is like a tea bag; you can't tell how strong she is until you put her in hot water.\n",
      "\n",
      "I'\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: A woman is like a tea bag;\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5af4e45d-efba-4358-8fc1-6ba25a0cf073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T16:02:14.444798Z",
     "iopub.status.busy": "2025-08-15T16:02:14.444465Z",
     "iopub.status.idle": "2025-08-15T16:02:15.862964Z",
     "shell.execute_reply": "2025-08-15T16:02:15.862426Z",
     "shell.execute_reply.started": "2025-08-15T16:02:14.444775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Outside of a dog, a book is man's best friend. Inside of a dog, it's too dark to read.\n",
      "\n",
      "-Groucho\n"
     ]
    }
   ],
   "source": [
    "text = \"Quote: Outside of a dog, a book is man's\"\n",
    "device = \"cuda:0\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ae2ad-7290-4aec-8047-ffcbaa10132d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
